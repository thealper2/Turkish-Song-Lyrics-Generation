{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tqdm\n",
    "import io\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "import emoji\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from textblob import TextBlob\n",
    "from nltk.util import ngrams\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, losses\n",
    "from tensorflow.keras.utils import to_categorical, plot_model, Sequence\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from IPython.display import Markdown, clear_output\n",
    "\n",
    "def bold(string):\n",
    "    display(Markdown(\"**\" + string + \"**\"))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/kaggle/input/turkish-song-lyrics/turkish_song_lyrics.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_stats(data):\n",
    "    bold(\" SHAPE \".center(50, \"#\"))\n",
    "    print(\"ROWS: {}\".format(data.shape[0]))\n",
    "    print(\"COLS: {}\".format(data.shape[1]))\n",
    "    bold(\" TYPES \".center(50, \"#\"))\n",
    "    print(data.dtypes)\n",
    "    bold(\" MISSING VALUES \".center(50, \"#\"))\n",
    "    print(data.isnull().sum())\n",
    "    bold(\" DUPLICATED VALUES \".center(50, \"#\"))\n",
    "    print(\"NUMBER OF DUPLICATED VALUES: {}\".format(data.duplicated().sum()))\n",
    "    #bold(\" DESCRIBE \".center(50, \"#\"))\n",
    "    #print(data.describe().T)\n",
    "    bold(\" MEMORY USAGE \".center(50, \"#\"))\n",
    "    buf = io.StringIO()\n",
    "    data.info(buf=buf)\n",
    "    info = buf.getvalue().split(\"\\n\")[-2].split(\":\")[1].strip()\n",
    "    print(\"Memory Usage: {}\".format(info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics = []\n",
    "\n",
    "for idx, row in tqdm.tqdm(df.iterrows()):\n",
    "    for lyric in row[\"lyrics\"].split(\"\\n\"):\n",
    "        lyrics.append(lyric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_df = pd.DataFrame({\"Lyrics\": lyrics})\n",
    "lyrics_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_df['Lyrics Cleaned'] = [token.lower() for token in lyrics_df['Lyrics']]\n",
    "lyrics_df['Lyrics Cleaned'] = lyrics_df['Lyrics Cleaned'].replace(r'http\\S+', '', regex=True).replace(r'www\\S+', '', regex=True)\n",
    "lyrics_df['Lyrics Cleaned'] = lyrics_df['Lyrics Cleaned'].apply(lambda x: re.sub('[0-9]+', '', x))\n",
    "lyrics_df['Lyrics Cleaned'] = lyrics_df['Lyrics Cleaned'].apply(lambda x: x.translate(x.maketrans('', '', string.punctuation)))\n",
    "lyrics_df['Lyrics Cleaned'] = lyrics_df['Lyrics Cleaned'].apply(lambda x: x.replace('\"', '').replace(\"’\", '').replace(\"'\", '').replace(\"”\", ''))\n",
    "lyrics_df['Lyrics Cleaned'] = lyrics_df['Lyrics Cleaned'].apply(lambda x: re.sub('\\S*@\\S*\\s?', '', x))\n",
    "lyrics_df['Lyrics Cleaned'] = lyrics_df['Lyrics Cleaned'].apply(lambda x: emoji.replace_emoji(x))\n",
    "lyrics_df['Lyrics Cleaned'] = lyrics_df['Lyrics Cleaned'].apply(lambda x: re.sub('<.*?>', '', x))\n",
    "#stop_words = [x.strip() for x in open('/kaggle/input/zemberekwords/stop-words.tr.txt','r', encoding=\"UTF8\").read().split('\\n')]\n",
    "#lyrics_df['Lyrics Cleaned'] = lyrics_df['Lyrics Cleaned'].apply(lambda text: ' '.join([word for word in text.split() if word.lower() not in stop_words]))\n",
    "#freq = pd.Series(' '.join(lyrics_df['Lyrics Cleaned']).split()).value_counts()\n",
    "#less_freq = list(freq[freq < 10].index)\n",
    "#lyrics_df['Lyrics Cleaned'] = lyrics_df['Lyrics Cleaned'].apply(lambda x: \" \".join(x for x in x.split() if x not in less_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_wordcloud(df, label):\n",
    "    tokens = ''\n",
    "    for token in df[label]:\n",
    "        tokens += token\n",
    "\n",
    "    wordcloud = WordCloud(background_color=\"white\", width=1200, height=800).generate(tokens)\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"{label} - Word Cloud\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_wordcloud(lyrics_df, \"Lyrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_wordcloud(lyrics_df, \"Lyrics Cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_ngrams(corpus, ngram, n):\n",
    "    vec = CountVectorizer(ngram_range=(ngram,ngram)).fit(corpus)\n",
    "    bow = vec.transform(corpus).sum(axis=0)\n",
    "    words_freq = sorted([(word, bow[0, idx]) for word, idx in vec.vocabulary_.items()], key=lambda x: x[1], reverse=True)[:n]\n",
    "    return words_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ngrams(ngram_df, ngram_name):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(data=ngram_df, x=\"Text\", height=\"Count\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.xlabel(ngram_name)\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(ngram_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams = count_ngrams(lyrics_df[\"Lyrics Cleaned\"], 1, 30)\n",
    "top_unigram = pd.DataFrame(unigrams, columns=['Text', \"Count\"])\n",
    "top_unigram.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ngrams(top_unigram, \"Unigrams\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = count_ngrams(lyrics_df[\"Lyrics Cleaned\"], 2, 30)\n",
    "top_bigram = pd.DataFrame(bigrams, columns=['Text', \"Count\"])\n",
    "top_bigram.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ngrams(top_bigram, \"Bigrams\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams = count_ngrams(lyrics_df[\"Lyrics Cleaned\"], 3, 30)\n",
    "top_trigram = pd.DataFrame(trigrams, columns=['Text', \"Count\"])\n",
    "top_trigram.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ngrams(top_trigram, \"Trigrams\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_df[\"Lyrics Cleaned\"] = lyrics_df[\"Lyrics Cleaned\"].apply(lambda x: \"startseq \" + str(x) + \" endseq\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 25\n",
    "X_col = 'Lyrics Cleaned'\n",
    "batch_size = 16\n",
    "vocab_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(lyrics_df[\"Lyrics Cleaned\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(lyrics_df, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_batch(batch, tokenizer, vocab_size, max_length):\n",
    "    X, y = list(), list()            \n",
    "    captions = batch[X_col].tolist()\n",
    "    for caption in captions:\n",
    "        seq = tokenizer.texts_to_sequences([caption])[0]\n",
    "        max_len = max_length if len(seq) > max_length else len(seq)\n",
    "        for i in range(1, max_len):\n",
    "            in_seq, out_seq = seq[:i], seq[i]\n",
    "            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "            X.append(in_seq)\n",
    "            y.append(out_seq)\n",
    "            \n",
    "    X, y = np.array(X), np.array(y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(df, tokenizer, vocab_size, max_length, batch_size):\n",
    "    n = len(df)\n",
    "    while True:\n",
    "        df = df.sample(frac=1).reset_index(drop=True)\n",
    "        for i in range(0, n, batch_size):\n",
    "            batch_df = df.iloc[i:i + batch_size]\n",
    "            X, y = preprocess_batch(batch_df, tokenizer, vocab_size, max_length)\n",
    "            yield X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = batch_generator(train_df, tokenizer, vocab_size, max_length, batch_size)\n",
    "test_gen = batch_generator(test_df, tokenizer, vocab_size, max_length, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = layers.Input(shape=(max_length,))\n",
    "embedding_layer = layers.Embedding(vocab_size, 64)(input_layer)\n",
    "bilstm_layer = layers.Bidirectional(layers.LSTM(100))(embedding_layer)\n",
    "output_layer = layers.Dense(vocab_size, activation='softmax')(bilstm_layer)\n",
    "\n",
    "model = models.Model(inputs=[input_layer], outputs=output_layer)\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=optimizers.Adam(learning_rate=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, show_layer_names=True, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', min_delta=0, patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = len(train_df) // batch_size\n",
    "validation_steps = len(test_df) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_gen, \n",
    "                    steps_per_epoch=steps_per_epoch, \n",
    "                    epochs=20, \n",
    "                    validation_data=test_gen, \n",
    "                    validation_steps=validation_steps, \n",
    "                    callbacks=[es])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend([\"train\", \"valid\"])\n",
    "plt.title(\"Loss Curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Greedy Search Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_search_predictions(text, model, tokenizer, max_length):\n",
    "    in_text = \"startseq \" + text\n",
    "    for _ in range(max_length):\n",
    "        sequence = pad_sequences([tokenizer.texts_to_sequences([in_text])[0]], max_length)\n",
    "        y_pred = np.argmax(model.predict(sequence, verbose=0), axis=1)[0]\n",
    "        \n",
    "        word = tokenizer.index_word.get(y_pred, None)\n",
    "        if not word or word == 'endseq':\n",
    "            break\n",
    "    \n",
    "        in_text += \" \" + word\n",
    "\n",
    "    return \" \".join([word for word in in_text.split() if word not in [\"startseq\", \"endseq\", \"<OOV>\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beam Search Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_predictions(text, beam_index = 3):\n",
    "    in_text = \"startseq \" + text\n",
    "    start_seq = tokenizer.texts_to_sequences([in_text])[0]\n",
    "    sequences = [[start_seq, 0.0]]\n",
    "    \n",
    "    while len(sequences[0][0]) < max_length:\n",
    "        all_candidates = []\n",
    "        for seq, score in sequences:\n",
    "            padded_seq = pad_sequences([seq], maxlen=max_length)\n",
    "            preds = model.predict(padded_seq, verbose=0)[0]\n",
    "            top_preds = np.argsort(preds)[-beam_index:]\n",
    "            all_candidates.extend([[seq + [w], score + preds[w]] for w in top_preds])\n",
    "        \n",
    "        sequences = sorted(all_candidates, key=lambda x: x[1])[-beam_index:]\n",
    "    \n",
    "    final_seq = sequences[-1][0]\n",
    "    final_caption = ' '.join([tokenizer.index_word[i] for i in final_seq if i not in [tokenizer.word_index[\"startseq\"], tokenizer.word_index.get(\"endseq\", 0), tokenizer.word_index.get(\"<OOV>\", 0)]])\n",
    "    \n",
    "    return final_caption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"neler olacak sonra\", \n",
    "             \"aman aman aman\", \n",
    "             \"ey ey ey\"]\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(\"\\nGreedy Search:\", greedy_search_predictions(sentence, model, tokenizer, max_length))\n",
    "    print(\"Beam Search:\", beam_search_predictions(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
